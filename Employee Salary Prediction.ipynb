{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ca564b-2f33-4283-887f-20ff3488bfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 02:04:07.733 No runtime found, using MemoryCacheStorageManager\n",
      "2025-07-23 02:04:07.746 No runtime found, using MemoryCacheStorageManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: 0.8490\n",
      "RandomForest: 0.8534\n",
      "KNN: 0.8323\n",
      "SVM: 0.8526\n",
      "GradientBoosting: 0.8616\n",
      "\n",
      "✅ Best model: GradientBoosting with accuracy 0.8616\n",
      "✅ Saved best model as models/best_model.pkl\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'fnlwgt'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 113\u001b[0m\n\u001b[0;32m     96\u001b[0m native_country \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39msidebar\u001b[38;5;241m.\u001b[39mselectbox(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNative Country\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnative-country\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m     98\u001b[0m input_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m: [age],\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkclass\u001b[39m\u001b[38;5;124m'\u001b[39m: [workclass],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnative-country\u001b[39m\u001b[38;5;124m'\u001b[39m: [native_country]\n\u001b[0;32m    111\u001b[0m })\n\u001b[1;32m--> 113\u001b[0m input_transformed \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mtransform(input_df)\n\u001b[0;32m    115\u001b[0m st\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### 🔍 Input Data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m st\u001b[38;5;241m.\u001b[39mdataframe(input_df)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1003\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1001\u001b[0m     diff \u001b[38;5;241m=\u001b[39m all_names \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[1;32m-> 1003\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: columns are missing: {'fnlwgt'}"
     ]
    }
   ],
   "source": [
    "# income_classifier_with_streamlit.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import streamlit as st\n",
    "import os\n",
    "\n",
    "# =============================\n",
    "# Data Loading and Preprocessing\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    data = pd.read_csv(\"adult 3.csv\")\n",
    "    data.replace('?', np.nan, inplace=True)\n",
    "    data.dropna(subset=['workclass', 'occupation', 'native-country'], inplace=True)\n",
    "    data = data[~data['workclass'].isin(['Without-pay', 'Never-worked'])]\n",
    "    data.drop(columns=['education', 'fnlwgt'], inplace=True)  # ✅ Removed fnlwgt\n",
    "    data = data[(data['age'] >= 17) & (data['age'] <= 75)]\n",
    "    data = data[(data['educational-num'] >= 5) & (data['educational-num'] <= 16)]\n",
    "    return data\n",
    "\n",
    "data = load_data()\n",
    "X = data.drop(columns='income')\n",
    "y = data['income']\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "])\n",
    "\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training and Selection\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"RandomForest\": RandomForestClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    results[name] = acc\n",
    "    print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "best_model_name = max(results, key=results.get)\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\n✅ Best model: {best_model_name} with accuracy {results[best_model_name]:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "joblib.dump(best_model, \"models/best_model.pkl\")\n",
    "print(\"✅ Saved best model as models/best_model.pkl\")\n",
    "\n",
    "# =============================\n",
    "# Streamlit App\n",
    "# =============================\n",
    "\n",
    "st.set_page_config(page_title=\"Income Classifier\", page_icon=\"💼\", layout=\"centered\")\n",
    "st.title(\"💼 Employee Income Classification\")\n",
    "st.markdown(\"Predict whether income is >50K or <=50K using demographic and job info.\")\n",
    "\n",
    "model = joblib.load('models/best_model.pkl')\n",
    "\n",
    "st.sidebar.header(\"📋 Input Features\")\n",
    "age = st.sidebar.slider(\"Age\", 17, 75, 30)\n",
    "edu_num = st.sidebar.slider(\"Education Level (numeric)\", 5, 16, 10)\n",
    "workclass = st.sidebar.selectbox(\"Workclass\", data['workclass'].unique())\n",
    "marital_status = st.sidebar.selectbox(\"Marital Status\", data['marital-status'].unique())\n",
    "occupation = st.sidebar.selectbox(\"Occupation\", data['occupation'].unique())\n",
    "relationship = st.sidebar.selectbox(\"Relationship\", data['relationship'].unique())\n",
    "race = st.sidebar.selectbox(\"Race\", data['race'].unique())\n",
    "gender = st.sidebar.selectbox(\"Gender\", data['gender'].unique())\n",
    "hours_per_week = st.sidebar.slider(\"Hours per Week\", 1, 80, 40)\n",
    "capital_gain = st.sidebar.number_input(\"Capital Gain\", 0, 100000, 0)\n",
    "capital_loss = st.sidebar.number_input(\"Capital Loss\", 0, 100000, 0)\n",
    "native_country = st.sidebar.selectbox(\"Native Country\", data['native-country'].unique())\n",
    "\n",
    "input_df = pd.DataFrame({\n",
    "    'age': [age],\n",
    "    'workclass': [workclass],\n",
    "    'educational-num': [edu_num],\n",
    "    'marital-status': [marital_status],\n",
    "    'occupation': [occupation],\n",
    "    'relationship': [relationship],\n",
    "    'race': [race],\n",
    "    'gender': [gender],\n",
    "    'capital-gain': [capital_gain],\n",
    "    'capital-loss': [capital_loss],\n",
    "    'hours-per-week': [hours_per_week],\n",
    "    'native-country': [native_country]\n",
    "})\n",
    "\n",
    "input_transformed = preprocessor.transform(input_df)\n",
    "\n",
    "st.write(\"### 🔍 Input Data\")\n",
    "st.dataframe(input_df)\n",
    "\n",
    "if st.button(\"Predict Income Class\"):\n",
    "    prediction = model.predict(input_transformed)\n",
    "    st.success(f\"✅ Prediction: {prediction[0]}\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"#### 📂 Batch Prediction\")\n",
    "uploaded_file = st.file_uploader(\"Upload a CSV for batch prediction\", type=\"csv\")\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    batch_data = pd.read_csv(uploaded_file)\n",
    "    batch_transformed = preprocessor.transform(batch_data)\n",
    "    batch_preds = model.predict(batch_transformed)\n",
    "    batch_data['PredictedClass'] = batch_preds\n",
    "    st.write(\"✅ Batch Predictions:\")\n",
    "    st.dataframe(batch_data.head())\n",
    "    csv = batch_data.to_csv(index=False).encode('utf-8')\n",
    "    st.download_button(\"Download Results\", csv, file_name='predicted_income.csv', mime='text/csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
